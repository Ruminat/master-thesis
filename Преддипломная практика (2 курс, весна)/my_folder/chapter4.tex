%!TeX root = ../My_thesis.tex


% .---|||___|||--- C H A P T E R ---|||___|||---. %
\chapter{Результаты и улучшение модели}
% .---|||___|||--- C H A P T E R ---|||___|||---. %


% .---|||___|||--- S E C T I O N ---|||___|||---. %
\section{Недостатки разработанной модели}
% .---|||___|||--- S E C T I O N ---|||___|||---. %


Разработанная модель обладает следующими недостатками:
\begin{itemize}%
  \item плохо справляется с большими предложениями\footnote{На самом деле, эта проблема частично может быть решена разбиением предложения по запятым и отдельному упрощению каждой части, однако лучше, конечно, было бы иметь решение, способное справляться с большими предложениями.},
  \item имеет относительно небольшой «словарный запас».
\end{itemize}

Обе проблемы вызваны довольно маленьким корпусом (50\,000 предложений для такой задачи "--- крайне малое количество).
Самым простым и очевидным решением в такой ситуации было бы взять больший корпус, однако в открытом доступе он попросту отсутствует.
Поэтому необходимо искать решение проблем в условиях очень небольшого корпуса.


% .---|||___|||--- S E C T I O N ---|||___|||---. %
\section{Варианты модификации модели}
% .---|||___|||--- S E C T I O N ---|||___|||---. %


Как мы уже говорили ранее, encoder в Transformer'е можно предобучить, чтобы он лучше кодировал входные последовательности слов предложений.
Сделать это можно, например, следующим образом:
\begin{itemize}%
  \item взять корпус из предложений на японском языке (к примеру, предложения с Википедии\footnote{Корпус с предложениями для предобучения модели может быть найден в репозитории модели~\cite{ServerGithub} "--- файл \texttt{modules/Dataset/wikipediaJp/data/wikipediaJp.csv}}), сопоставить каждое предложение самому себе (то есть упрощение будет вестись в исходные предложения);
  \item обучить таким образом модель;
  \item получить encoder, который имеет какое-то представление о японском языке.
\end{itemize}

Может возникнуть вопрос: как же мы улучшим модель, если будем обучаться на корпусе, в котором никакого упрощения совсем нет?
Секрет кроется в том, encoder не отвечает за само упрощение "--- он лишь кодирует предложения в матрицы чисел.
Поэтому мы можем взять этот encoder и дальше уже обучать изначальную модель с его внедрением.
В данной работе мы попробуем использовать 2 стратегии внедрения encoder'а в изначальную модель:
\begin{enumerate}[1.]%
  \item взять предобученную модель «как есть» и обучить её на корпусе с упрощёнными предложениями;
  \item взять изначальную модель и положить в неё предобученный encoder, сгенерировав остальные коэффициенты.
\end{enumerate}

Стоит также отметить, что просто взяв предобученный encoder и положив его в Transformer, мы многого не добьёмся "--- модель попросту обучится на корпусе с упрощёнными предложениями и пользы от предобученнго encoder'а мы не получим.
Чтобы этого не произошло мы уменьшим learning rate для слоёв encoder'а (в данной работе "--- в 5 раз\footnote{Число 5 было найдено подбором: сначала была попытка с learning rate в 10, потом в 2, потом в 4 и, наконец, в 5 "--- что дало лучший результат для метрик.}).
В PyTorch это можно сделать следующим образом:
\begin{minted}[tabsize=2, mathescape, linenos, xleftmargin=20pt, fontsize=\scriptsize]{python}
encoder = []
rest = []
for name, param in transformer.named_parameters():
  if "encoder" in name:
    encoder.append(param)
  else:
    rest.append(param)

optimizer = torch.optim.Adam(
  [{'params': encoder}, {'params': rest}],
  # ... параметры обучения
)
# здесь мы уменьшаем learning rate у encoder'а в 5 раз
optimizer.param_groups[0]['lr'] = LEARNING_RATE / 5
\end{minted}

Таким образом мы значительно ограничиваем возможность изменения параметров encoder'а при обучении.
Что позволяет нам воспользоваться преимуществом его предобучения.


% .---|||___|||--- S E C T I O N ---|||___|||---. %
\section{Метрики для оценки модели упрощения}
% .---|||___|||--- S E C T I O N ---|||___|||---. %


Для перевода текстов (в том числе и упрощения) довольно часто используют метрку BLEU~\cite{BLEU}.
В оригинальной статье Папинени и др. показали наличие корреляции данной метрики с сохранением грамматики и смысла переведённых предложений.

Однако есть и более специфичная метрика, разработанная специально для автоматического упрощения текстов "--- SARI~\cite{SARI}.
Она, по сравнению BLEU, лучше коррелирует с упрощением предложений, а также с сохранением лексической и структурной частей предложений.


% .---|||___|||--- S E C T I O N ---|||___|||---. %
\section{Результаты}
% .---|||___|||--- S E C T I O N ---|||___|||---. %


Результаты метрик BLUE и SARI для изначальной модели и вариантов её улучшения представлены в~\taref{metrics}.

\begin{table}[H]% Пример оформления таблицы
  \centering\small
  \caption{Метрики полученных моделей}
  \label{metrics}
    \begin{tabular}{|l|l|l|}
      \hline
      \textbf{Модель} & \textbf{BLEU} & \textbf{SARI} \\ \hline
      Transformer & 45{,}63 & 63{,}17 \\ \hline
      Pretrained Transformer & 50{,}78 & 67{,}33 \\ \hline
      Pretrained Encoder & 46{,}87 & 64{,}27 \\ \hline
    \end{tabular}
    \normalsize
\end{table}

В~\taref{metrics} введены следующие обозначения:
\begin{itemize}%
  \item Transformer "--- изначальная модель;
  \item Pretrained Transformer "--- предобученная модель, которую дообучаем, замедляя обучение encoder'а;
  \item Pretrained Encoder "--- изначальная модель, в которой заменяем encoder на предобученный, замедляя его обучение.
\end{itemize}

По~\taref{metrics} видно, что наиболее успешной оказалсь модель Pretrained Transformer "--- улучшение по сравнению с изначальной моделью на 5{,}15\% и 4{,}16\% для BLEU и SARI соответственно.
Это может говорить о том, что предобучение положительно сказывается и на decoder'е, так как упрощение в основном оставляет предложение в исходном виде, за исключением упрощённых его частей.

Рассмотрим пример предложения из корпуса для тестирования, упрощение которого улучшилось благодаря модификции модели "--- см.~\firef{simplificationComparison}\footnote{Здесь также происходит упрощение слова \jp{ますます} (на \jp{さらに} "--- более распространённое слово), однако обе модели упростили его одинакого, поэтому не будем заострять на этом внимание.}.

\begin{figure}[H]%
  \centering
  \begin{tabular}{l}
    (1) исходное предложение \\  
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{は}}{wa}
    \yubi{\jp{内気}}{uchiki}
    \yubi{\jp{な}}{na}
    \yubi{\jp{ので}}{node}
    \yubi{\jp{、}}{}
    \yubi{\jp{ますます}}{masumasu}
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{が}}{ga}
    \yubi{\jp{好き}}{suki}
    \yubi{\jp{だ}}{da} \\ 
    пер. "--- она робкая, из-за чего я люблю её ещё больше \\ 
    (2) изначальная модель \\ 
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{は}}{wa}
    \yubi{\jp{気}}{\textbf{ki}}
    \yubi{\jp{が}}{\textbf{ga}}
    \yubi{\jp{悪く}}{\textbf{waruku}}
    \yubi{\jp{思う}}{\textbf{omou}}
    \yubi{\jp{ので}}{\textbf{node}}
    \yubi{\jp{、}}{}
    \yubi{\jp{さら}}{sara}
    \yubi{\jp{に}}{ni}
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{が}}{ga}
    \yubi{\jp{好き}}{suki}
    \yubi{\jp{だ}}{da} \\ 
    пер. "--- она \textbf{плохо себя чувствует}, из-за чего я люблю её ещё больше \\ 
    (3) модифицированная модель (Pretrained Transformer) \\  
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{は}}{wa}
    \yubi{\jp{気}}{\textbf{ki}}
    \yubi{\jp{が}}{\textbf{ga}}
    \yubi{\jp{弱い}}{\textbf{yowai}}
    \yubi{\jp{ので}}{node}
    \yubi{\jp{、}}{}
    \yubi{\jp{さら}}{sara}
    \yubi{\jp{に}}{ni}
    \yubi{\jp{彼女}}{kanojo}
    \yubi{\jp{が}}{ga}
    \yubi{\jp{好き}}{suki}
    \yubi{\jp{だ}}{da} \\
    пер. "--- она \textbf{скромная}, из-за чего я люблю её ещё больше \\ 
  \end{tabular}
  \caption{Пример улучшения упрощения предложения}
  \label{simplificationComparison}
\end{figure}


% % .---|||___|||--- S E C T I O N ---|||___|||---. %
% \section{Примеры упрощения разработанной системой}
% % .---|||___|||--- S E C T I O N ---|||___|||---. %


% Посмотрим на несколько примеров упрощения разработанной системой, а также улучшенной версии.


